<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 5.14.0"/><style data-href="/styles.d6c075ab698e745c0517.css" data-identity="gatsby-global-css">body{background-color:#f4f4f4;margin:0;padding:0}</style><link rel="preconnect" href="https://www.googletagmanager.com"/><link rel="dns-prefetch" href="https://www.googletagmanager.com"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NKZN8BZHQV"></script><script>
      
      function gaOptout(){document.cookie=disableStr+'=true; expires=Thu, 31 Dec 2099 23:59:59 UTC;path=/',window[disableStr]=!0}var gaProperty='G-NKZN8BZHQV',disableStr='ga-disable-'+gaProperty;document.cookie.indexOf(disableStr+'=true')>-1&&(window[disableStr]=!0);
      if(true) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-NKZN8BZHQV', {"anonymize_ip":true,"send_page_view":false});
      }
      </script></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><main style="padding:3rem 2rem;font-family:&#x27;Courier New&#x27;, Courier, monospace;background-color:#f4f4f4;color:#333;max-width:900px;margin:0 auto;border-radius:8px"><header style="margin-bottom:2rem"><div style="margin-bottom:1rem"><a href="/" style="text-decoration:none;font-size:1.5rem;font-weight:bold;color:#333;font-family:&#x27;Courier New&#x27;, Courier, monospace">*_</a></div><h1 style="font-size:2.5rem;margin:0;font-weight:normal"></h1><p style="font-size:1.1rem;color:#555;line-height:1.6;font-style:italic">Dive into this topic as part of the Spirit Riddle training.</p></header><article style="font-size:1rem;line-height:1.8"><h1>Algorithms and Models Terminology for Search Engines</h1>
<h2>Text Processing</h2>
<ul>
<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>: A statistical measure that evaluates the importance of a word in a document relative to a collection of documents.</li>
<li><strong>Cosine Similarity</strong>: A metric used to measure the cosine of the angle between two non-zero vectors, often representing document similarity.</li>
<li><strong>Jaccard Similarity</strong>: Measures the overlap between two sets, used to calculate similarity between documents or terms.</li>
<li><strong>Bag of Words (BoW)</strong>: A representation of text data where the frequency of words is used without considering grammar or order.</li>
<li><strong>Word Embeddings</strong>: Dense vector representations of words in a continuous space, capturing semantic relationships.</li>
</ul>
<h2>Graph-Based Algorithms</h2>
<ul>
<li><strong>PageRank</strong>: An algorithm that ranks web pages by analyzing the link structure of the web, assigning higher scores to pages with more or higher-quality links.</li>
<li><strong>HITS (Hyperlink-Induced Topic Search)</strong>: A graph-based algorithm that identifies hubs (pages pointing to many authorities) and authorities (pages pointed to by many hubs).</li>
<li><strong>Graph Traversal</strong>:
<ul>
<li><strong>Depth-First Search (DFS)</strong>: Explores as far as possible along a branch before backtracking.</li>
<li><strong>Breadth-First Search (BFS)</strong>: Explores all nodes at the current level before moving deeper.</li>
</ul>
</li>
<li><strong>Shortest Path Algorithms</strong>:
<ul>
<li><strong>Dijkstra's Algorithm</strong>: Finds the shortest path from a single source to all nodes in a graph.</li>
<li><em><em>A</em> Algorithm</em>*: An optimization of Dijkstra's algorithm using heuristics for faster pathfinding.</li>
</ul>
</li>
<li><strong>Connected Components</strong>: Identifies groups of connected nodes in a graph.</li>
</ul>
<h2>Clustering Models</h2>
<ul>
<li><strong>K-Means Clustering</strong>: Partitions data into K clusters by minimizing the variance within each cluster.</li>
<li><strong>Hierarchical Clustering</strong>: Creates a tree-like structure of clusters, useful for visualizing relationships.</li>
<li><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong>: Groups points based on density, identifying clusters of arbitrary shape and handling outliers.</li>
</ul>
<h2>Ranking Models</h2>
<ul>
<li><strong>BM25</strong>: A probabilistic model used for ranking documents based on term frequency and document length.</li>
<li><strong>Learning to Rank</strong>: Machine learning models that combine multiple features to rank documents or items.</li>
</ul>
<h2>Dimensionality Reduction</h2>
<ul>
<li><strong>Singular Value Decomposition (SVD)</strong>: Decomposes a matrix into components to reduce dimensionality, commonly used in Latent Semantic Analysis.</li>
<li><strong>Principal Component Analysis (PCA)</strong>: Reduces dimensionality by finding the principal components that capture the most variance in data.</li>
</ul>
<h2>Probabilistic Models</h2>
<ul>
<li><strong>Naive Bayes Classifier</strong>: A probabilistic algorithm based on Bayes' theorem, used for text classification.</li>
<li><strong>Latent Dirichlet Allocation (LDA)</strong>: A generative probabilistic model for topic modeling in text data.</li>
<li><strong>Hidden Markov Models (HMM)</strong>: Models sequences of observations and hidden states, often used in language modeling.</li>
</ul>
<h2>Optimization Techniques</h2>
<ul>
<li><strong>Gradient Descent</strong>: An iterative algorithm to minimize a loss function by updating model parameters in the direction of steepest descent.</li>
<li><strong>Regularization</strong>: A method to prevent overfitting by penalizing complex models.</li>
</ul>
<h2>Information Retrieval Models</h2>
<ul>
<li><strong>Vector Space Model</strong>: Represents documents and queries as vectors in a multidimensional space, enabling similarity computation.</li>
<li><strong>Boolean Retrieval Model</strong>: Uses Boolean operators (AND, OR, NOT) to match documents to queries.</li>
</ul>
<h2>Neural Network Models for Search</h2>
<ul>
<li><strong>Transformer Models</strong>: Deep learning models that process sequential data, such as text, using self-attention mechanisms.</li>
<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: A transformer-based model that understands context by processing text bidirectionally.</li>
<li><strong>Embedding-Based Retrieval</strong>: Uses dense vector representations to retrieve semantically similar documents.</li>
</ul>
<p>This terminology encompasses key mathematical and algorithmic foundations essential for search engine technology.</p></article><footer style="margin-top:3rem;text-align:center;font-size:0.9rem;color:#888"><p>Â© <!-- -->2024<!-- --> Spirit Riddle. All rights reserved.</p><p style="font-size:0.8rem;color:#aaa">Explore more about Spirit Riddle on our<!-- --> <a href="/" style="text-decoration:none;color:#555;font-weight:bold">Homepage</a>.</p></footer></main></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/training/google-web-search-engineer-math/Algorithms_and_Models_Terminology/";/*]]>*/</script><!-- slice-start id="_gatsby-scripts-1" -->
          <script
            id="gatsby-chunk-mapping"
          >
            window.___chunkMapping="{\"app\":[\"/app-0e9e5de2d9cfac0badcf.js\"],\"component---src-pages-404-js\":[\"/component---src-pages-404-js-7f8b8bb9939a73239a43.js\"],\"component---src-pages-blog-components-footer-jsx\":[\"/component---src-pages-blog-components-footer-jsx-d57e360cdf0844ac1a16.js\"],\"component---src-pages-blog-components-header-jsx\":[\"/component---src-pages-blog-components-header-jsx-d1981f2e29334dfef53e.js\"],\"component---src-pages-blog-memory-algorithmic-cognitive-enhancer-js\":[\"/component---src-pages-blog-memory-algorithmic-cognitive-enhancer-js-06f31f5f6b45835f5e08.js\"],\"component---src-pages-blog-universal-service-adapter-model-lov-js\":[\"/component---src-pages-blog-universal-service-adapter-model-lov-js-bd19d6422b94e02efccc.js\"],\"component---src-pages-index-js\":[\"/component---src-pages-index-js-9d4dfc5f65dbd80f0a99.js\"],\"component---src-pages-training-google-web-search-engineer-math-index-js\":[\"/component---src-pages-training-google-web-search-engineer-math-index-js-553844f254cd461e4b44.js\"],\"component---src-pages-training-index-js\":[\"/component---src-pages-training-index-js-14b3ab7f6137bbb14b8a.js\"],\"component---src-templates-markdown-template-js\":[\"/component---src-templates-markdown-template-js-4b908192a6b567c5f3a0.js\"]}";
          </script>
        <script>window.___webpackCompilationHash="632e40ca2b2541e23d3d";</script><script src="/webpack-runtime-730af717d26cbdfcdafb.js" async></script><script src="/framework-f13d21eee99ae197812c.js" async></script><script src="/app-0e9e5de2d9cfac0badcf.js" async></script><!-- slice-end id="_gatsby-scripts-1" --></body></html>