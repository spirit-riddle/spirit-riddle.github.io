{"componentChunkName":"component---src-templates-markdown-template-js","path":"/training/google-web-search-engineer-math/Linear_Algebra_Terminology/","result":{"data":{"markdownRemark":{"html":"<h1>Linear Algebra Terminology for Search Engines and Optimization Algorithms</h1>\n<h2>Matrix Operations</h2>\n<ul>\n<li><strong>Addition</strong>: Combining two matrices by adding their corresponding elements.</li>\n<li><strong>Multiplication</strong>: Combining two matrices to form a new matrix, often used to model transformations or relationships.</li>\n<li><strong>Transpose</strong>: Flipping a matrix over its diagonal, converting rows into columns.</li>\n<li><strong>Inverse</strong>: A matrix that, when multiplied with the original matrix, yields the identity matrix; used in solving systems of equations.</li>\n</ul>\n<h2>Vector Spaces</h2>\n<ul>\n<li><strong>Vector</strong>: A mathematical object with magnitude and direction, often used to represent data points or terms in a search engine.</li>\n<li><strong>Basis Vectors</strong>: A set of vectors that define a coordinate system for a vector space.</li>\n<li><strong>Linear Independence</strong>: A property where no vector in a set is a linear combination of the others, crucial for understanding dimensions of data.</li>\n</ul>\n<h2>Rank of a Matrix</h2>\n<ul>\n<li><strong>Rank</strong>: The number of linearly independent rows or columns in a matrix, indicating the amount of meaningful information.</li>\n</ul>\n<h2>Eigenvalues and Eigenvectors</h2>\n<ul>\n<li><strong>Eigenvalue</strong>: A scalar that represents how a transformation scales an eigenvector.</li>\n<li><strong>Eigenvector</strong>: A vector that remains in the same direction after a transformation, used in ranking algorithms like PageRank to identify importance in networks.</li>\n</ul>\n<h2>Singular Value Decomposition (SVD)</h2>\n<ul>\n<li><strong>SVD</strong>: A matrix factorization technique that decomposes a matrix into three components (U, Σ, Vᵀ). Used in Latent Semantic Analysis to reduce dimensionality and uncover latent relationships in data.</li>\n</ul>\n<h2>Dot Product</h2>\n<ul>\n<li><strong>Dot Product</strong>: The multiplication of two vectors resulting in a scalar. Used to measure similarity between two data points in vector space.</li>\n</ul>\n<h2>Norms</h2>\n<ul>\n<li><strong>L2 Norm (Euclidean Distance)</strong>: Measures the \"length\" of a vector in space, used to quantify similarity or difference between data points.</li>\n<li><strong>L1 Norm (Manhattan Distance)</strong>: Measures the \"taxicab\" distance between two points in a grid-like path.</li>\n</ul>\n<h2>Projection</h2>\n<ul>\n<li><strong>Projection</strong>: Mapping a vector onto another vector or subspace, often used to reduce dimensions while retaining key features.</li>\n</ul>\n<h2>Orthogonality</h2>\n<ul>\n<li><strong>Orthogonal Vectors</strong>: Vectors that are perpendicular to each other, indicating no similarity. Orthogonal matrices preserve distances and are useful for optimization.</li>\n</ul>\n<h2>Diagonalization</h2>\n<ul>\n<li><strong>Diagonalization</strong>: Converting a matrix into a diagonal form using its eigenvalues, simplifying computations.</li>\n</ul>\n<h2>Outer Product</h2>\n<ul>\n<li><strong>Outer Product</strong>: A matrix formed by multiplying one vector as a column and another as a row, used in algorithms like SVD.</li>\n</ul>\n<h2>Sparse Matrices</h2>\n<ul>\n<li><strong>Sparse Matrix</strong>: A matrix with a large number of zero elements, commonly used in representing large datasets like term-document matrices in search engines.</li>\n</ul>\n<h2>Row and Column Space</h2>\n<ul>\n<li><strong>Row Space</strong>: The set of all possible linear combinations of the row vectors of a matrix.</li>\n<li><strong>Column Space</strong>: The set of all possible linear combinations of the column vectors of a matrix. Both are key for understanding solutions to linear systems.</li>\n</ul>\n<h2>QR Factorization</h2>\n<ul>\n<li><strong>QR Factorization</strong>: Decomposing a matrix into an orthogonal matrix (Q) and an upper triangular matrix (R), often used in numerical optimization.</li>\n</ul>","frontmatter":{"title":""}}},"pageContext":{"slug":"google-web-search-engineer-math/Linear_Algebra_Terminology"}},"staticQueryHashes":[],"slicesMap":{}}