{"componentChunkName":"component---src-templates-markdown-template-js","path":"/training/google-web-search-engineer-math/Algorithms_and_Models_Terminology/","result":{"data":{"markdownRemark":{"html":"<h1>Algorithms and Models Terminology for Search Engines</h1>\n<h2>Text Processing</h2>\n<ul>\n<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>: A statistical measure that evaluates the importance of a word in a document relative to a collection of documents.</li>\n<li><strong>Cosine Similarity</strong>: A metric used to measure the cosine of the angle between two non-zero vectors, often representing document similarity.</li>\n<li><strong>Jaccard Similarity</strong>: Measures the overlap between two sets, used to calculate similarity between documents or terms.</li>\n<li><strong>Bag of Words (BoW)</strong>: A representation of text data where the frequency of words is used without considering grammar or order.</li>\n<li><strong>Word Embeddings</strong>: Dense vector representations of words in a continuous space, capturing semantic relationships.</li>\n</ul>\n<h2>Graph-Based Algorithms</h2>\n<ul>\n<li><strong>PageRank</strong>: An algorithm that ranks web pages by analyzing the link structure of the web, assigning higher scores to pages with more or higher-quality links.</li>\n<li><strong>HITS (Hyperlink-Induced Topic Search)</strong>: A graph-based algorithm that identifies hubs (pages pointing to many authorities) and authorities (pages pointed to by many hubs).</li>\n<li><strong>Graph Traversal</strong>:\n<ul>\n<li><strong>Depth-First Search (DFS)</strong>: Explores as far as possible along a branch before backtracking.</li>\n<li><strong>Breadth-First Search (BFS)</strong>: Explores all nodes at the current level before moving deeper.</li>\n</ul>\n</li>\n<li><strong>Shortest Path Algorithms</strong>:\n<ul>\n<li><strong>Dijkstra's Algorithm</strong>: Finds the shortest path from a single source to all nodes in a graph.</li>\n<li><em><em>A</em> Algorithm</em>*: An optimization of Dijkstra's algorithm using heuristics for faster pathfinding.</li>\n</ul>\n</li>\n<li><strong>Connected Components</strong>: Identifies groups of connected nodes in a graph.</li>\n</ul>\n<h2>Clustering Models</h2>\n<ul>\n<li><strong>K-Means Clustering</strong>: Partitions data into K clusters by minimizing the variance within each cluster.</li>\n<li><strong>Hierarchical Clustering</strong>: Creates a tree-like structure of clusters, useful for visualizing relationships.</li>\n<li><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong>: Groups points based on density, identifying clusters of arbitrary shape and handling outliers.</li>\n</ul>\n<h2>Ranking Models</h2>\n<ul>\n<li><strong>BM25</strong>: A probabilistic model used for ranking documents based on term frequency and document length.</li>\n<li><strong>Learning to Rank</strong>: Machine learning models that combine multiple features to rank documents or items.</li>\n</ul>\n<h2>Dimensionality Reduction</h2>\n<ul>\n<li><strong>Singular Value Decomposition (SVD)</strong>: Decomposes a matrix into components to reduce dimensionality, commonly used in Latent Semantic Analysis.</li>\n<li><strong>Principal Component Analysis (PCA)</strong>: Reduces dimensionality by finding the principal components that capture the most variance in data.</li>\n</ul>\n<h2>Probabilistic Models</h2>\n<ul>\n<li><strong>Naive Bayes Classifier</strong>: A probabilistic algorithm based on Bayes' theorem, used for text classification.</li>\n<li><strong>Latent Dirichlet Allocation (LDA)</strong>: A generative probabilistic model for topic modeling in text data.</li>\n<li><strong>Hidden Markov Models (HMM)</strong>: Models sequences of observations and hidden states, often used in language modeling.</li>\n</ul>\n<h2>Optimization Techniques</h2>\n<ul>\n<li><strong>Gradient Descent</strong>: An iterative algorithm to minimize a loss function by updating model parameters in the direction of steepest descent.</li>\n<li><strong>Regularization</strong>: A method to prevent overfitting by penalizing complex models.</li>\n</ul>\n<h2>Information Retrieval Models</h2>\n<ul>\n<li><strong>Vector Space Model</strong>: Represents documents and queries as vectors in a multidimensional space, enabling similarity computation.</li>\n<li><strong>Boolean Retrieval Model</strong>: Uses Boolean operators (AND, OR, NOT) to match documents to queries.</li>\n</ul>\n<h2>Neural Network Models for Search</h2>\n<ul>\n<li><strong>Transformer Models</strong>: Deep learning models that process sequential data, such as text, using self-attention mechanisms.</li>\n<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: A transformer-based model that understands context by processing text bidirectionally.</li>\n<li><strong>Embedding-Based Retrieval</strong>: Uses dense vector representations to retrieve semantically similar documents.</li>\n</ul>\n<p>This terminology encompasses key mathematical and algorithmic foundations essential for search engine technology.</p>","frontmatter":{"title":""}}},"pageContext":{"slug":"google-web-search-engineer-math/Algorithms_and_Models_Terminology"}},"staticQueryHashes":[],"slicesMap":{}}