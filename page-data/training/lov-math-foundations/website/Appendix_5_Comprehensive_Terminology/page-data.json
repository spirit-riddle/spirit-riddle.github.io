{"componentChunkName":"component---src-templates-markdown-template-js","path":"/training/lov-math-foundations/website/Appendix_5_Comprehensive_Terminology/","result":{"data":{"markdownRemark":{"html":"<p><small>*_ Spirit Riddle Presents</small></p>\n<h1>Comprehensive Terminology for Algorithms, Graph Theory, Linear Algebra, and Probability</h1>\n<p>This packet includes the following:</p>\n<ul>\n<li><strong>Graph Theory</strong>: Concepts and algorithms essential for understanding networks and connectivity.</li>\n<li><strong>Algorithms and Models</strong>: Foundational techniques for text processing, clustering, and ranking.</li>\n<li><strong>Linear Algebra</strong>: Operations, eigenvalues, and decompositions critical for optimization and data transformations.</li>\n<li><strong>Probability and Statistics</strong>: Tools for data sampling, inference, and modeling uncertainty in real-world applications.</li>\n</ul>\n<h2>Table of Contents</h2>\n<ul>\n<li><a href=\"#graph-theory-terminology-for-search-engines\">Graph Theory Terminology for Search Engines</a></li>\n<li><a href=\"#algorithms-and-models-terminology-for-search-engines\">Algorithms and Models Terminology for Search Engines</a></li>\n<li><a href=\"#linear-algebra-terminology-for-search-engines-and-optimization-algorithms\">Linear Algebra Terminology for Search Engines and Optimization Algorithms</a></li>\n<li><a href=\"#probability-and-statistics-terminology-for-ranking-algorithms-and-web-search\">Probability and Statistics Terminology for Ranking Algorithms and Web Search</a></li>\n<li><a href=\"#final-notes\">Final Notes</a></li>\n</ul>\n<br/>\n<br/>\n<h2>Graph Theory Terminology for Search Engines</h2>\n<h3>Fundamental Concepts</h3>\n<ul>\n<li><strong>Graph</strong>: A collection of nodes (vertices) and edges connecting them, used to represent relationships and structures.</li>\n<li><strong>Directed Graph (Digraph)</strong>: A graph where edges have a direction, often used in web page link analysis.</li>\n<li><strong>Undirected Graph</strong>: A graph where edges have no direction, representing bidirectional relationships.</li>\n</ul>\n<h3>Key Properties</h3>\n<ul>\n<li><strong>Node (Vertex)</strong>: A fundamental unit of a graph, representing entities such as web pages or data points.</li>\n<li><strong>Edge</strong>: A connection between two nodes, which can be directed or undirected.</li>\n<li><strong>Degree</strong>:\n<ul>\n<li><strong>In-Degree</strong>: Number of edges coming into a node.</li>\n<li><strong>Out-Degree</strong>: Number of edges leaving a node.</li>\n</ul>\n</li>\n<li><strong>Weighted Graph</strong>: A graph where edges have weights representing costs, distances, or probabilities.</li>\n</ul>\n<h3>Graph Algorithms</h3>\n<ul>\n<li><strong>Graph Traversal</strong>:\n<ul>\n<li><strong>Depth-First Search (DFS)</strong>: Explores as far as possible along a branch before backtracking.</li>\n<li><strong>Breadth-First Search (BFS)</strong>: Explores all nodes at the current level before moving deeper.</li>\n</ul>\n</li>\n<li><strong>Shortest Path</strong>:\n<ul>\n<li><strong>Dijkstra's Algorithm</strong>: Finds the shortest path in a weighted graph.</li>\n<li><em><em>A</em> Algorithm</em>*: Optimized pathfinding using heuristics.</li>\n</ul>\n</li>\n<li><strong>Minimum Spanning Tree (MST)</strong>:\n<ul>\n<li><strong>Prim's Algorithm</strong>: Builds an MST by starting from a node and adding the smallest edge.</li>\n<li><strong>Kruskal's Algorithm</strong>: Builds an MST by sorting edges and adding them incrementally.</li>\n</ul>\n</li>\n</ul>\n<h3>Advanced Concepts</h3>\n<ul>\n<li><strong>Adjacency Matrix</strong>: A square matrix used to represent a graph, where each element indicates the presence or absence of an edge.</li>\n<li><strong>Adjacency List</strong>: A list representation of a graph, where each node has a list of its adjacent nodes.</li>\n<li><strong>Connectivity</strong>:\n<ul>\n<li><strong>Connected Graph</strong>: A graph where there is a path between every pair of nodes.</li>\n<li><strong>Strongly Connected Components (SCCs)</strong>: Subsets of a directed graph where every node is reachable from every other node within the subset.</li>\n</ul>\n</li>\n</ul>\n<h3>Applications in Search Engines</h3>\n<ul>\n<li><strong>PageRank</strong>: A graph-based algorithm that ranks web pages by analyzing the link structure of the web.</li>\n<li><strong>HITS Algorithm</strong>: Identifies hubs (pages pointing to many authorities) and authorities (pages pointed to by many hubs).</li>\n<li><strong>Graph Traversal for Indexing</strong>: Techniques like BFS and DFS are used to crawl and index web pages.</li>\n<li><strong>Weighted Graphs for Ranking</strong>: Models relationships between pages and computes relevance scores based on link weights.</li>\n</ul>\n<h3>Visualization</h3>\n<ul>\n<li><strong>Graph Plotting</strong>: Visualizing nodes and edges to understand relationships and structures.</li>\n<li><strong>Force-Directed Layouts</strong>: A technique for graph visualization where edges act as springs and nodes repel each other.</li>\n</ul>\n<p>This terminology provides the foundational lingo for discussing graph theory in the context of search engine algorithms and web structures.</p>\n<h2>Algorithms and Models Terminology for Search Engines</h2>\n<h3>Text Processing</h3>\n<ul>\n<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>: A statistical measure that evaluates the importance of a word in a document relative to a collection of documents.</li>\n<li><strong>Cosine Similarity</strong>: A metric used to measure the cosine of the angle between two non-zero vectors, often representing document similarity.</li>\n<li><strong>Jaccard Similarity</strong>: Measures the overlap between two sets, used to calculate similarity between documents or terms.</li>\n<li><strong>Bag of Words (BoW)</strong>: A representation of text data where the frequency of words is used without considering grammar or order.</li>\n<li><strong>Word Embeddings</strong>: Dense vector representations of words in a continuous space, capturing semantic relationships.</li>\n</ul>\n<h3>Graph-Based Algorithms</h3>\n<ul>\n<li><strong>PageRank</strong>: An algorithm that ranks web pages by analyzing the link structure of the web, assigning higher scores to pages with more or higher-quality links.</li>\n<li><strong>HITS (Hyperlink-Induced Topic Search)</strong>: A graph-based algorithm that identifies hubs (pages pointing to many authorities) and authorities (pages pointed to by many hubs).</li>\n<li><strong>Graph Traversal</strong>:\n<ul>\n<li><strong>Depth-First Search (DFS)</strong>: Explores as far as possible along a branch before backtracking.</li>\n<li><strong>Breadth-First Search (BFS)</strong>: Explores all nodes at the current level before moving deeper.</li>\n</ul>\n</li>\n<li><strong>Shortest Path Algorithms</strong>:\n<ul>\n<li><strong>Dijkstra's Algorithm</strong>: Finds the shortest path from a single source to all nodes in a graph.</li>\n<li><em><em>A</em> Algorithm</em>*: An optimization of Dijkstra's algorithm using heuristics for faster pathfinding.</li>\n</ul>\n</li>\n<li><strong>Connected Components</strong>: Identifies groups of connected nodes in a graph.</li>\n</ul>\n<h3>Clustering Models</h3>\n<ul>\n<li><strong>K-Means Clustering</strong>: Partitions data into K clusters by minimizing the variance within each cluster.</li>\n<li><strong>Hierarchical Clustering</strong>: Creates a tree-like structure of clusters, useful for visualizing relationships.</li>\n<li><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong>: Groups points based on density, identifying clusters of arbitrary shape and handling outliers.</li>\n</ul>\n<h3>Ranking Models</h3>\n<ul>\n<li><strong>BM25</strong>: A probabilistic model used for ranking documents based on term frequency and document length.</li>\n<li><strong>Learning to Rank</strong>: Machine learning models that combine multiple features to rank documents or items.</li>\n</ul>\n<h3>Dimensionality Reduction</h3>\n<ul>\n<li><strong>Singular Value Decomposition (SVD)</strong>: Decomposes a matrix into components to reduce dimensionality, commonly used in Latent Semantic Analysis.</li>\n<li><strong>Principal Component Analysis (PCA)</strong>: Reduces dimensionality by finding the principal components that capture the most variance in data.</li>\n</ul>\n<h3>Probabilistic Models</h3>\n<ul>\n<li><strong>Naive Bayes Classifier</strong>: A probabilistic algorithm based on Bayes' theorem, used for text classification.</li>\n<li><strong>Latent Dirichlet Allocation (LDA)</strong>: A generative probabilistic model for topic modeling in text data.</li>\n<li><strong>Hidden Markov Models (HMM)</strong>: Models sequences of observations and hidden states, often used in language modeling.</li>\n</ul>\n<h3>Optimization Techniques</h3>\n<ul>\n<li><strong>Gradient Descent</strong>: An iterative algorithm to minimize a loss function by updating model parameters in the direction of steepest descent.</li>\n<li><strong>Regularization</strong>: A method to prevent overfitting by penalizing complex models.</li>\n</ul>\n<h3>Information Retrieval Models</h3>\n<ul>\n<li><strong>Vector Space Model</strong>: Represents documents and queries as vectors in a multidimensional space, enabling similarity computation.</li>\n<li><strong>Boolean Retrieval Model</strong>: Uses Boolean operators (AND, OR, NOT) to match documents to queries.</li>\n</ul>\n<h3>Neural Network Models for Search</h3>\n<ul>\n<li><strong>Transformer Models</strong>: Deep learning models that process sequential data, such as text, using self-attention mechanisms.</li>\n<li><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: A transformer-based model that understands context by processing text bidirectionally.</li>\n<li><strong>Embedding-Based Retrieval</strong>: Uses dense vector representations to retrieve semantically similar documents.</li>\n</ul>\n<p>This terminology encompasses key mathematical and algorithmic foundations essential for search engine technology.</p>\n<h2>Linear Algebra Terminology for Search Engines and Optimization Algorithms</h2>\n<h3>Matrix Operations</h3>\n<ul>\n<li><strong>Addition</strong>: Combining two matrices by adding their corresponding elements.</li>\n<li><strong>Multiplication</strong>: Combining two matrices to form a new matrix, often used to model transformations or relationships.</li>\n<li><strong>Transpose</strong>: Flipping a matrix over its diagonal, converting rows into columns.</li>\n<li><strong>Inverse</strong>: A matrix that, when multiplied with the original matrix, yields the identity matrix; used in solving systems of equations.</li>\n</ul>\n<h3>Vector Spaces</h3>\n<ul>\n<li><strong>Vector</strong>: A mathematical object with magnitude and direction, often used to represent data points or terms in a search engine.</li>\n<li><strong>Basis Vectors</strong>: A set of vectors that define a coordinate system for a vector space.</li>\n<li><strong>Linear Independence</strong>: A property where no vector in a set is a linear combination of the others, crucial for understanding dimensions of data.</li>\n</ul>\n<h3>Rank of a Matrix</h3>\n<ul>\n<li><strong>Rank</strong>: The number of linearly independent rows or columns in a matrix, indicating the amount of meaningful information.</li>\n</ul>\n<h3>Eigenvalues and Eigenvectors</h3>\n<ul>\n<li><strong>Eigenvalue</strong>: A scalar that represents how a transformation scales an eigenvector.</li>\n<li><strong>Eigenvector</strong>: A vector that remains in the same direction after a transformation, used in ranking algorithms like PageRank to identify importance in networks.</li>\n</ul>\n<h3>Singular Value Decomposition (SVD)</h3>\n<ul>\n<li><strong>SVD</strong>: A matrix factorization technique that decomposes a matrix into three components (U, Σ, Vᵀ). Used in Latent Semantic Analysis to reduce dimensionality and uncover latent relationships in data.</li>\n</ul>\n<h3>Dot Product</h3>\n<ul>\n<li><strong>Dot Product</strong>: The multiplication of two vectors resulting in a scalar. Used to measure similarity between two data points in vector space.</li>\n</ul>\n<h3>Norms</h3>\n<ul>\n<li><strong>L2 Norm (Euclidean Distance)</strong>: Measures the \"length\" of a vector in space, used to quantify similarity or difference between data points.</li>\n<li><strong>L1 Norm (Manhattan Distance)</strong>: Measures the \"taxicab\" distance between two points in a grid-like path.</li>\n</ul>\n<h3>Projection</h3>\n<ul>\n<li><strong>Projection</strong>: Mapping a vector onto another vector or subspace, often used to reduce dimensions while retaining key features.</li>\n</ul>\n<h3>Orthogonality</h3>\n<ul>\n<li><strong>Orthogonal Vectors</strong>: Vectors that are perpendicular to each other, indicating no similarity. Orthogonal matrices preserve distances and are useful for optimization.</li>\n</ul>\n<h3>Diagonalization</h3>\n<ul>\n<li><strong>Diagonalization</strong>: Converting a matrix into a diagonal form using its eigenvalues, simplifying computations.</li>\n</ul>\n<h3>Outer Product</h3>\n<ul>\n<li><strong>Outer Product</strong>: A matrix formed by multiplying one vector as a column and another as a row, used in algorithms like SVD.</li>\n</ul>\n<h3>Sparse Matrices</h3>\n<ul>\n<li><strong>Sparse Matrix</strong>: A matrix with a large number of zero elements, commonly used in representing large datasets like term-document matrices in search engines.</li>\n</ul>\n<h3>Row and Column Space</h3>\n<ul>\n<li><strong>Row Space</strong>: The set of all possible linear combinations of the row vectors of a matrix.</li>\n<li><strong>Column Space</strong>: The set of all possible linear combinations of the column vectors of a matrix. Both are key for understanding solutions to linear systems.</li>\n</ul>\n<h3>QR Factorization</h3>\n<ul>\n<li><strong>QR Factorization</strong>: Decomposing a matrix into an orthogonal matrix (Q) and an upper triangular matrix (R), often used in numerical optimization.</li>\n</ul>\n<h2>Probability and Statistics Terminology for Ranking Algorithms and Web Search</h2>\n<h3>Basic Probability</h3>\n<ul>\n<li><strong>Probability</strong>: A measure of the likelihood that an event will occur, ranging from 0 (impossible) to 1 (certain).</li>\n<li><strong>Independent Events</strong>: Two events where the occurrence of one does not affect the other.</li>\n<li><strong>Conditional Probability</strong>: The probability of one event occurring given that another event has already occurred.</li>\n<li><strong>Bayes' Theorem</strong>: A formula that relates the conditional and marginal probabilities of random events, used in Bayesian inference.</li>\n</ul>\n<h3>Distributions</h3>\n<ul>\n<li><strong>Normal Distribution</strong>: A continuous probability distribution that is symmetric around the mean, forming a bell-shaped curve. Used in many natural phenomena.</li>\n<li><strong>Binomial Distribution</strong>: Describes the number of successes in a fixed number of binary (yes/no) trials.</li>\n<li><strong>Poisson Distribution</strong>: Models the number of events occurring within a fixed interval of time or space.</li>\n</ul>\n<h3>Expectation and Variance</h3>\n<ul>\n<li><strong>Expectation (Mean)</strong>: The average value of a random variable over many trials.</li>\n<li><strong>Variance</strong>: Measures the spread of a random variable around its mean.</li>\n<li><strong>Standard Deviation</strong>: The square root of the variance, representing the average distance from the mean.</li>\n</ul>\n<h3>Bayesian Inference</h3>\n<ul>\n<li><strong>Bayesian Inference</strong>: A method of statistical inference in which Bayes' theorem is used to update probabilities as more evidence becomes available.</li>\n<li><strong>Prior Probability</strong>: The initial probability of an event before new evidence is considered.</li>\n<li><strong>Posterior Probability</strong>: The updated probability of an event after considering new evidence.</li>\n</ul>\n<h3>Hypothesis Testing</h3>\n<ul>\n<li><strong>Null Hypothesis (H₀)</strong>: A statement that there is no effect or no difference, used as a baseline in statistical testing.</li>\n<li><strong>Alternative Hypothesis (H₁)</strong>: A statement that contradicts the null hypothesis, suggesting an effect or difference.</li>\n<li><strong>P-Value</strong>: The probability of obtaining results at least as extreme as the observed results, assuming the null hypothesis is true.</li>\n<li><strong>Confidence Interval</strong>: A range of values that is likely to contain the true value of an unknown parameter.</li>\n</ul>\n<h3>Regression Analysis</h3>\n<ul>\n<li><strong>Linear Regression</strong>: A method to model the relationship between a dependent variable and one or more independent variables.</li>\n<li><strong>Logistic Regression</strong>: Used to model binary outcomes (e.g., true/false, yes/no).</li>\n</ul>\n<h3>Information Gain</h3>\n<ul>\n<li><strong>Entropy</strong>: A measure of the uncertainty or randomness in a set of data.</li>\n<li><strong>Mutual Information</strong>: Measures the reduction in uncertainty about one variable given knowledge of another.</li>\n</ul>\n<h3>Markov Models</h3>\n<ul>\n<li><strong>Markov Chain</strong>: A stochastic model describing a sequence of possible events where the probability of each event depends only on the state of the previous event.</li>\n<li><strong>Transition Matrix</strong>: A matrix that represents probabilities of transitioning from one state to another in a Markov chain.</li>\n</ul>\n<h3>Random Variables</h3>\n<ul>\n<li><strong>Random Variable</strong>: A variable whose value is subject to randomness, often categorized as discrete or continuous.</li>\n<li><strong>Probability Density Function (PDF)</strong>: Describes the likelihood of a continuous random variable taking on a specific value.</li>\n<li><strong>Cumulative Distribution Function (CDF)</strong>: Describes the probability that a random variable is less than or equal to a certain value.</li>\n</ul>\n<h3>Sampling and Estimation</h3>\n<ul>\n<li><strong>Sampling</strong>: Selecting a subset of data from a population for analysis.</li>\n<li><strong>Bias</strong>: A systematic error introduced into sampling or estimation.</li>\n<li><strong>Maximum Likelihood Estimation (MLE)</strong>: A method of estimating the parameters of a statistical model by maximizing the likelihood function.</li>\n</ul>\n<h3>Correlation and Dependence</h3>\n<ul>\n<li><strong>Correlation Coefficient</strong>: A measure of the linear relationship between two variables, ranging from -1 to 1.</li>\n<li><strong>Covariance</strong>: A measure of how two random variables vary together.</li>\n</ul>\n<h3>Statistical Models in Search</h3>\n<ul>\n<li><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>: A statistical measure used to evaluate the importance of a word in a document relative to a corpus.</li>\n<li><strong>Latent Dirichlet Allocation (LDA)</strong>: A probabilistic model used for topic modeling in text analysis.</li>\n</ul>\n<p>This list captures the essential probability and statistics concepts that underpin ranking algorithms and web search relevance models.</p>\n<h2>Final Notes</h2>\n<p>This combined terminology provides a foundational understanding of algorithms, graph theory, linear algebra, and probability essential for search engines, optimization, and modern data science applications.</p>","frontmatter":{"title":""}}},"pageContext":{"slug":"lov-math-foundations/website/Appendix_5_Comprehensive_Terminology"}},"staticQueryHashes":[],"slicesMap":{}}